from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pinecone
from sentence_transformers import SentenceTransformer

# ğŸ”§ FastAPI App Initialization
app = FastAPI(
    title="Pinecone Chatbot API",
    description="Semantic chatbot powered by sentence transformers and Pinecone",
    version="1.0"
)

# ğŸ›¡ï¸ CORS Settings (optional)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Replace with your domain for stricter control
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸŒ² Pinecone Initialization
pinecone.init(
    api_key="your-api-key",           # ğŸ”‘ Replace with your Pinecone API key
    environment="your-environment"    # ğŸŒ Example: "gcp-starter", "us-west1-gcp"
)

# ğŸ” Create Index Object
index = pinecone.Index("your-index-name")  # ğŸ”— Replace with your actual index name

# ğŸ”  Load Embedding Model
embedder = SentenceTransformer("all-MiniLM-L6-v2")  # â© Lightweight & fast

# ğŸ§¾ Input Schema
class ChatQuery(BaseModel):
    query: str

# ğŸ’¬ Chat Endpoint
@app.post("/chat")
async def chat_endpoint(query: ChatQuery):
    query_text = query.query
    query_vector = embedder.encode(query_text).tolist()
    
    search_result = index.query(
        vector=query_vector,
        top_k=3,
        include_metadata=True
    )

    # ğŸ“š Extract Answers
    answers = [match["metadata"]["text"] for match in search_result["matches"]]
    return {"question": query_text, "answer": answers}

# ğŸ  Root Endpoint
@app.get("/")
def root():
    return {"message": "ğŸš€ Pinecone Chatbot API is running!"}